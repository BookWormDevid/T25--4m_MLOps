import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix, classification_report
import joblib
from pathlib import Path
import json
import os
import mlflow

# –ü—É—Ç–∏
MODEL_PATH = Path("models/model.joblib")
PROCESSED_DATA = Path(r"C:\Users\Koraku\Documents\mlops-flight-delay\data\processed\processed.csv")
REPORT_PATH = Path("reports/eval.json")


def evaluate():
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö
    if not MODEL_PATH.exists():
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MODEL_PATH}")
        return

    if not PROCESSED_DATA.exists():
        print(f"‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {PROCESSED_DATA}")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö
    model = joblib.load(MODEL_PATH)
    df = pd.read_csv(PROCESSED_DATA)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = df[["DepHour", "IsWeekend"]]
    y = (df["ArrDelay"] > 15).astype(int)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"üìä –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} samples")
    print(f"üéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–µ:")
    print(y_test.value_counts().sort_index())

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    f1_score = 2 * (precision * recall) / (precision + recall)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ç—á–µ—Ç–∞ –¥–ª—è JSON
    evaluation_report = {
        "test_set_info": {
            "samples_count": len(y_test),
            "test_size": 0.2,
            "random_state": 42
        },
        "metrics": {
            "roc_auc": float(roc_auc),
            "precision": float(precision),
            "recall": float(recall),
            "accuracy": float(accuracy),
            "f1_score": float(f1_score)
        },
        "confusion_matrix": {
            "true_negative": int(tn),
            "false_positive": int(fp),
            "false_negative": int(fn),
            "true_positive": int(tp),
            "matrix": cm.tolist()
        },
        "predictions": {
            "total_predictions": len(y_pred),
            "class_0_predictions": int((y_pred == 0).sum()),
            "class_1_predictions": int((y_pred == 1).sum())
        }
    }

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ JSON
    os.makedirs(REPORT_PATH.parent, exist_ok=True)
    with open(REPORT_PATH, 'w', encoding='utf-8') as f:
        json.dump(evaluation_report, f, indent=2, ensure_ascii=False)

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
    try:
        mlflow.set_experiment("flight_delay_evaluation")

        with mlflow.start_run(run_name="model_evaluation"):
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ü–µ–Ω–∫–∏
            mlflow.log_params({
                "test_size": 0.2,
                "random_state": 42,
                "evaluation_data": "full_dataset_split"
            })

            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
            mlflow.log_metrics({
                "roc_auc": roc_auc,
                "precision": precision,
                "recall": recall,
                "accuracy": accuracy,
                "f1_score": f1_score
            })

            # –õ–æ–≥–∏—Ä—É–µ–º confusion matrix –∫–∞–∫ –º–µ—Ç—Ä–∏–∫–∏
            mlflow.log_metrics({
                "true_negative": tn,
                "false_positive": fp,
                "false_negative": fn,
                "true_positive": tp
            })

            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
            mlflow.log_artifact(REPORT_PATH)

            print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ MLflow")

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ MLflow: {e}")

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 50)
    print("üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ù–ê –¢–ï–°–¢–ï")
    print("=" * 50)
    print(f"ROC-AUC:   {roc_auc:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"F1-Score:  {f1_score:.4f}")

    print(f"\nüìã Confusion Matrix:")
    print(f"TN: {tn} | FP: {fp}")
    print(f"FN: {fn} | TP: {tp}")

    print(f"\nüíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {REPORT_PATH}")

    return evaluation_report


if __name__ == "__main__":
    evaluate()
